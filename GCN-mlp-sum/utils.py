import numpy as np
import pickle as pkl
import networkx as nx
import scipy.sparse as sp
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.colors import colorConverter as cc
from scipy.sparse.linalg.eigen.arpack import eigsh
import sys
import torch

def count_params(model):
  return sum(p.numel() for p in model.parameters() if p.requires_grad)

def encode_onehot(labels):
  classes = set(labels)
  classes_dict = {c: np.identity(len(classes))[i, :] for i, c in
          enumerate(classes)}
  labels_onehot = np.array(list(map(classes_dict.get, labels)),
               dtype=np.int32)
  return labels_onehot


def load_data(path="data/cora/", dataset="cora"):
  """Load citation network dataset (cora only for now)"""
  print('Loading {} dataset...'.format(dataset))

  idx_features_labels = np.genfromtxt("{}{}.content".format(path, dataset),
                    dtype=np.dtype(str))
  features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)
  labels = encode_onehot(idx_features_labels[:, -1])

  # build graph
  idx = np.array(idx_features_labels[:, 0], dtype=np.int32)
  idx_map = {j: i for i, j in enumerate(idx)}
  edges_unordered = np.genfromtxt("{}{}.cites".format(path, dataset),
                  dtype=np.int32)
  edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),
           dtype=np.int32).reshape(edges_unordered.shape)
  adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),
            shape=(labels.shape[0], labels.shape[0]),
            dtype=np.float32)

  # build symmetric adjacency matrix
  adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)

  features = normalize(features)
  adj = adj + sp.eye(adj.shape[0])

  idx_train = range(140)
  idx_val = range(200, 500)
  idx_test = range(500, 1500)

  features = torch.FloatTensor(np.array(features.todense()))
  labels = torch.LongTensor(np.where(labels)[1])
  adj = sparse_mx_to_torch_sparse_tensor(adj)

  idx_train = torch.LongTensor(idx_train)
  idx_val = torch.LongTensor(idx_val)
  idx_test = torch.LongTensor(idx_test)

  return adj, features, labels, idx_train, idx_val, idx_test
  

def load_data_tf(dataset_str):
  """
  Loads input data from gcn/data directory

  ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;
  ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;
  ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances
    (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;
  ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;
  ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;
  ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;
  ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict
    object;
  ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.

  All objects above must be saved using python pickle module.

  :param dataset_str: Dataset name
  :return: All data input files loaded (as well the training/test data).
  """
  names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']
  objects = []
  for i in range(len(names)):
    with open("data/ind.{}.{}".format(dataset_str, names[i]), 'rb') as f:
      if sys.version_info > (3, 0):
        objects.append(pkl.load(f, encoding='latin1'))
      else:
        objects.append(pkl.load(f))

  x, y, tx, ty, allx, ally, graph = tuple(objects)
  test_idx_reorder = parse_index_file("data/ind.{}.test.index".format(dataset_str))
  test_idx_range = np.sort(test_idx_reorder)

  if dataset_str == 'citeseer':
    # Fix citeseer dataset (there are some isolated nodes in the graph)
    # Find isolated nodes, add them as zero-vecs into the right position
    test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)
    tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))
    tx_extended[test_idx_range-min(test_idx_range), :] = tx
    tx = tx_extended
    ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))
    ty_extended[test_idx_range-min(test_idx_range), :] = ty
    ty = ty_extended

  features = sp.vstack((allx, tx)).tolil()
  features[test_idx_reorder, :] = features[test_idx_range, :]
  adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))

  labels = np.vstack((ally, ty))
  labels[test_idx_reorder, :] = labels[test_idx_range, :]

  idx_test = test_idx_range.tolist()
  idx_train = range(len(y))
  idx_val = range(len(y), len(y)+500)

  train_mask = sample_mask(idx_train, labels.shape[0])
  val_mask = sample_mask(idx_val, labels.shape[0])
  test_mask = sample_mask(idx_test, labels.shape[0])

  y_train = np.zeros(labels.shape)
  y_val = np.zeros(labels.shape)
  y_test = np.zeros(labels.shape)
  y_train[train_mask, :] = labels[train_mask, :]
  y_val[val_mask, :] = labels[val_mask, :]
  y_test[test_mask, :] = labels[test_mask, :]

  return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask
  

def load_data_new(dataset_str):
  """
  Loads input data from gcn/data directory

  ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;
  ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;
  ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances
    (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;
  ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;
  ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;
  ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;
  ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict
    object;
  ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.

  All objects above must be saved using python pickle module.

  :param dataset_str: Dataset name
  :return: All data input files loaded (as well the training/test data).
  """
  names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']
  objects = []
  for i in range(len(names)):
    with open("../data/ind.{}.{}".format(dataset_str, names[i]), 'rb') as f:
      if sys.version_info > (3, 0):
        objects.append(pkl.load(f, encoding='latin1'))
      else:
        objects.append(pkl.load(f))

  x, y, tx, ty, allx, ally, graph = tuple(objects)
  test_idx_reorder = parse_index_file("../data/ind.{}.test.index".format(dataset_str))
  test_idx_range = np.sort(test_idx_reorder)

  if dataset_str == "citeseer":
    # Fix citeseer dataset (there are some isolated nodes in the graph)
    # Find isolated nodes, add them as zero-vecs into the right position
    test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)
    tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))
    tx_extended[test_idx_range-min(test_idx_range), :] = tx
    tx = tx_extended
    ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))
    ty_extended[test_idx_range-min(test_idx_range), :] = ty
    ty = ty_extended

  features = sp.vstack((allx, tx)).tolil()
  features[test_idx_reorder, :] = features[test_idx_range, :]
  adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))

  labels = np.vstack((ally, ty))
  labels[test_idx_reorder, :] = labels[test_idx_range, :]
  
  features = normalize(features)
  adj = adj + sp.eye(adj.shape[0])

  idx_test = test_idx_range.tolist()
  idx_train = range(len(y))
  idx_val = range(len(y), len(y)+500)

  features = torch.FloatTensor(np.array(features.todense()))
  # The old code did a where
  #labels = torch.LongTensor(np.where(labels)[1])
  # I'm doing argmax since it is stable where all labels are zero (which happens in citesser)
  labels = torch.LongTensor(np.argmax(labels,axis=1))
  adj = sparse_mx_to_torch_sparse_tensor(adj)
  idx_train = torch.LongTensor(idx_train)
  idx_val = torch.LongTensor(idx_val)
  idx_test = torch.LongTensor(idx_test)

  return adj, features, labels, idx_train, idx_val, idx_test


def normalize(mx):
  """Row-normalize sparse matrix"""
  rowsum = np.array(mx.sum(1))
  r_inv = np.power(rowsum, -1).flatten()
  r_inv[np.isinf(r_inv)] = 0.
  r_mat_inv = sp.diags(r_inv)
  mx = r_mat_inv.dot(mx)
  return mx


def accuracy(output, labels):
  preds = output.max(1)[1].type_as(labels)
  correct = preds.eq(labels).double()
  correct = correct.sum()
  return correct / len(labels)


def sparse_mx_to_torch_sparse_tensor(sparse_mx):
  """Convert a scipy sparse matrix to a torch sparse tensor."""
  sparse_mx = sparse_mx.tocoo().astype(np.float32)
  indices = torch.from_numpy(
    np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))
  values = torch.from_numpy(sparse_mx.data)
  shape = torch.Size(sparse_mx.shape)
  return torch.sparse.FloatTensor(indices, values, shape)
  
  
  
# TF utils

def parse_index_file(filename):
  """Parse index file."""
  index = []
  for line in open(filename):
    index.append(int(line.strip()))
  return index

def sample_mask(idx, l):
  """Create mask."""
  mask = np.zeros(l)
  mask[idx] = 1
  return np.array(mask, dtype=np.bool)

# Adapted from https://studywolf.wordpress.com/2017/11/21/matplotlib-legends-for-mean-and-confidence-interval-plots/
def plot_mean_and_std(x, mean, std, color_mean=None, marker_mean=None, color_shading=None, label=None):
  # plot the shaded range of the confidence intervals
  plt.fill_between(x, mean+std, mean-std,
           color=color_shading, alpha=0.5)
  # plot the mean on top
  plt.plot(x, mean, color=color_mean, marker=marker_mean, label=label)

class LegendObject(object):
  def __init__(self, facecolor='red', edgecolor='white', dashed=False):
    self.facecolor = facecolor
    self.edgecolor = edgecolor
    self.dashed = dashed

  def legend_artist(self, legend, orig_handle, fontsize, handlebox):
    x0, y0 = handlebox.xdescent, handlebox.ydescent
    width, height = handlebox.width, handlebox.height
    patch = mpatches.Rectangle(
      # create a rectangle that is filled with color
      [x0, y0], width, height, facecolor=self.facecolor,
      # and whose edges are the faded color
      edgecolor=self.edgecolor, lw=3)
    handlebox.add_artist(patch)

    # if we're creating the legend for a dashed line,
    # manually add the dash in to our rectangle
    if self.dashed:
      patch1 = mpatches.Rectangle(
        [x0 + 2*width/5, y0], width/5, height, facecolor=self.edgecolor,
        transform=handlebox.get_transform())
      handlebox.add_artist(patch1)

    return patch
